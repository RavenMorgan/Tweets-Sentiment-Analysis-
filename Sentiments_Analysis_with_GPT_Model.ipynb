{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxbB+5yWZ/u0b8VwvmXdX2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RavenMorgan/Tweets-Sentiment-Analysis-/blob/main/Sentiments_Analysis_with_GPT_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiments_Analysis_with_GPT_Model"
      ],
      "metadata": {
        "id": "aeq_lvbKcB9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/RavenMorgan/Tweets-Sentiment-Analysis-.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxaKUp-ee45e",
        "outputId": "970bcdc3-3815-40e1-80c7-30f15e8444a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Tweets-Sentiment-Analysis-'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 38 (delta 11), reused 5 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (38/38), 7.09 MiB | 9.50 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import re"
      ],
      "metadata": {
        "id": "VjYwn_lWfDQp",
        "outputId": "5feef0bc-6b70-4f1e-f769-bf4dbfc37936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREPROCESSING"
      ],
      "metadata": {
        "id": "wLpgz1oCg4Dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Tweets-Sentiment-Analysis-/training.1600000.processed.noemoticon.csv',\n",
        "                 encoding = 'latin',header=None)\n",
        "\n",
        "# On supprime les colonnes inutiles\n",
        "df = df.drop([1,2,3,4], axis=1)\n",
        "# On garde le sentiment et le texte\n",
        "df.columns= ['sentiment','text']\n",
        "\n",
        "df.sentiment = df.sentiment.apply(lambda x: \"positif\" if x == 4 else \"négatif\")"
      ],
      "metadata": {
        "id": "SIAM1ow-e9J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text, stem=False):\n",
        "    stemmer = SnowballStemmer('english')\n",
        "\n",
        "    # Cleaning corpus\n",
        "    text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
        "\n",
        "    # Delete stopwords\n",
        "    stop_words = stopwords.words('english')\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "df.text = df.text.apply(lambda x: preprocess(x))"
      ],
      "metadata": {
        "id": "juswgflGfQeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA SPLITTING"
      ],
      "metadata": {
        "id": "06TCG_6ngn2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SIZE = 0.8\n",
        "MAX_NB_WORDS = 100000"
      ],
      "metadata": {
        "id": "lbgD1Rypgpmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.text, df.sentiment, test_size=1-TRAIN_SIZE,\n",
        "                                         random_state=7)\n",
        "\n",
        "print(\"Training X Shape:\",X_train.shape)\n",
        "print(\"Testing X Shape:\",X_test.shape)\n",
        "print(\"Training X Shape:\",y_train.shape)\n",
        "print(\"Testing X Shape:\",y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjfsYevWgsbU",
        "outputId": "4f37740d-5d71-48fe-be60-4067cdee22a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training X Shape: (1280000,)\n",
            "Testing X Shape: (320000,)\n",
            "Training X Shape: (1280000,)\n",
            "Testing X Shape: (320000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPEN AI\n",
        "\n",
        "L'API Open AI demande une clé d'authentification ainsi qu'un abonnement payant pour l'utiliser."
      ],
      "metadata": {
        "id": "etI-1EvBcf0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYZNVEd-e4Tb"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "LwCdBSdLdw8z",
        "outputId": "5b9d791c-2d5c-49b3-9cf0-2db9f19f90d5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b387899697e6>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Analyze sentiments for the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0msentiment_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Text: {text}, Predicted Sentiment: {sentiment_result}, Actual Sentiment: {label}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-b387899697e6>\u001b[0m in \u001b[0;36manalyze_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0manalyze_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Sentiment analysis: {text}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     response = openai.Completion.create(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-davinci-002\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "api_key = \"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\"\n",
        "openai.api_key = api_key\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    prompt = f\"Sentiment analysis: {text}\"\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-002\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=50  # You can adjust this based on your needs\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Analyze sentiments for the training set\n",
        "for text, label in zip(X_train, y_train):\n",
        "    sentiment_result = analyze_sentiment(text)\n",
        "    print(f\"Text: {text}, Predicted Sentiment: {sentiment_result}, Actual Sentiment: {label}\")\n",
        "\n",
        "# Example usage for a single test instance\n",
        "text_to_analyze = X_train[0]\n",
        "sentiment_result = analyze_sentiment(text_to_analyze)\n",
        "actual_sentiment = y_test[0]\n",
        "print(f\"Text: {text_to_analyze}, Predicted Sentiment: {sentiment_result}, Actual Sentiment: {actual_sentiment}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FREE GPT 4 FREE\n",
        "\n",
        "G4F est un une librairie GPT qui utilise des modèles pré-entrainés."
      ],
      "metadata": {
        "id": "rInVVGIJEcTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U g4f"
      ],
      "metadata": {
        "id": "MHkYH-qGEcCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import g4f\n",
        "\n",
        "g4f.debug.logging = True  # Enable debug logging\n",
        "g4f.debug.check_version = False  # Disable automatic version checking\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "  prompt = f\"Please analyse this sentiment by determining if the next text is either positive or negative: {text}\"\n",
        "  print(prompt)\n",
        "\n",
        "  response = g4f.ChatCompletion.create(\n",
        "      model=g4f.models.gpt_4,\n",
        "      # provider=g4f.Provider.ChatBase,\n",
        "      messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "      stream=True,\n",
        "  )\n",
        "  return response\n",
        "\n",
        "# Analyze sentiments for the training set\n",
        "# for text, label in zip(X_train, y_train):\n",
        "#     sentiment_result = analyze_sentiment(text)\n",
        "#     print(f\"Text: {text}, Predicted Sentiment: {sentiment_result}, Actual Sentiment: {label}\")\n",
        "\n",
        "# Example usage for a single test instance\n",
        "text_to_analyze = X_train[0]\n",
        "response = analyze_sentiment(text_to_analyze)\n",
        "actual_sentiment = y_train[0]\n",
        "\n",
        "for message in response:\n",
        "  print(f\"\\nText: {text_to_analyze}\")\n",
        "  print(message, flush=True, end='')\n",
        "  print(f\"\\nActual Sentiment: {actual_sentiment}\\n\")\n"
      ],
      "metadata": {
        "id": "pBj6tXZtE-rM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}